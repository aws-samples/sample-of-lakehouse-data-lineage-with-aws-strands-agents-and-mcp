import os
import sys
import asyncio
import uuid
import json
import time
from datetime import datetime
from dotenv import load_dotenv

# Load environment variables first
load_dotenv()

# Ensure AWS region is set
if not os.getenv('AWS_DEFAULT_REGION'):
    os.environ['AWS_DEFAULT_REGION'] = 'us-west-2'
if not os.getenv('AWS_REGION'):
    os.environ['AWS_REGION'] = 'us-west-2'

import streamlit as st
from openai import OpenAI

# Page configuration
st.set_page_config(
    page_title="Lakehouse E2E data lineage with AWS Strands agents and MCP",
    page_icon="ğŸ”",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        text-align: center;
        color: #1f77b4;
        margin-bottom: 2rem;
    }
    .tool-status {
        padding: 0.5rem;
        border-radius: 0.5rem;
        margin: 0.25rem 0;
    }
    .tool-success {
        background-color: #d4edda;
        border: 1px solid #c3e6cb;
        color: #155724;
    }
    .tool-error {
        background-color: #f8d7da;
        border: 1px solid #f5c6cb;
        color: #721c24;
    }
    .tool-running {
        background-color: #fff3cd;
        border: 1px solid #ffeaa7;
        color: #856404;
    }
    .timeout-warning {
        background-color: #fff3cd;
        border: 1px solid #ffeaa7;
        color: #856404;
        padding: 1rem;
        border-radius: 0.5rem;
        margin: 1rem 0;
    }
    .sidebar-section {
        margin-bottom: 1.5rem;
    }
</style>
""", unsafe_allow_html=True)

# Initialize session state
if 'tool_calls' not in st.session_state:
    st.session_state.tool_calls = []
if 'query_history' not in st.session_state:
    st.session_state.query_history = []
if 'neptune_status' not in st.session_state:
    st.session_state.neptune_status = None
if 'mcp_client' not in st.session_state:
    st.session_state.mcp_client = None
if 'qwen_client' not in st.session_state:
    st.session_state.qwen_client = None

def check_neptune_connection():
    """Check Neptune connection status"""
    try:
        neptune_endpoint = os.environ.get('NEPTUNE_ENDPOINT')
        if not neptune_endpoint:
            return False, "NEPTUNE_ENDPOINT ç¯å¢ƒå˜é‡æœªè®¾ç½®"
        
        try:
            from awslabs.amazon_neptune_mcp_server.server import get_status
            status = get_status()
            return True, f"Neptune MCP æœåŠ¡æ­£å¸¸: {status}"
        except ImportError:
            return False, "Neptune MCP æœåŠ¡å™¨æ¨¡å—æœªå®‰è£…"
        except Exception as mcp_error:
            return True, f"Neptune ç«¯ç‚¹å·²é…ç½®ï¼Œä½†MCPæ£€æŸ¥å¤±è´¥: {str(mcp_error)[:50]}..."
    except Exception as e:
        return False, f"è¿æ¥æ£€æŸ¥å¤±è´¥: {str(e)}"

def get_mcp_client():
    """Get or create MCP client"""
    try:
        from mcp import stdio_client, StdioServerParameters
        from strands.tools.mcp import MCPClient
        
        mcp_client = MCPClient(lambda: stdio_client(
            StdioServerParameters(
                command="python", 
                args=["-m", "awslabs.amazon_neptune_mcp_server.server"],
                env={
                    'NEPTUNE_ENDPOINT': os.environ.get('NEPTUNE_ENDPOINT'),
                    'AWS_REGION': os.environ.get('AWS_REGION'),
                    'AWS_DEFAULT_REGION': os.environ.get('AWS_DEFAULT_REGION')
                }
            )
        ))
        return mcp_client
    except Exception as e:
        st.error(f"MCPå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        return None

def get_qwen_client():
    """Get or create Qwen API client"""
    if st.session_state.qwen_client is None:
        try:
            api_key = os.getenv('DASHSCOPE_API_KEY')
            if not api_key:
                st.error("è¯·è®¾ç½®ç¯å¢ƒå˜é‡ DASHSCOPE_API_KEY")
                return None
            
            st.session_state.qwen_client = OpenAI(
                api_key=api_key,
                base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
            )
        except Exception as e:
            st.error(f"Qwenå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            return None
    
    return st.session_state.qwen_client

def create_enhanced_claude_agent(system_prompt, mcp_client):
    """Create enhanced agent with comprehensive data source analysis and impact assessment"""
    try:
        qwen_client = get_qwen_client()
        if not qwen_client:
            raise ValueError("Qwen APIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥")
        
        class EnhancedClaudeAgent:
            def __init__(self, qwen_client, mcp_client, system_prompt):
                self.qwen_client = qwen_client
                self.mcp_client = mcp_client
                self.system_prompt = system_prompt
                self.tool_call_log = []
            
            def call_mcp_tool(self, tool_name, arguments=None):
                """Call MCP tool and log the process - backend logging + sidebar monitoring"""
                try:
                    # Backend logging
                    self.tool_call_log.append(f"è°ƒç”¨å·¥å…·: {tool_name}")
                    if arguments:
                        self.tool_call_log.append(f"å‚æ•°: {arguments}")
                    
                    # Sidebar monitoring - add to session state
                    st.session_state.tool_calls.append({
                        'tool': tool_name,
                        'status': 'running',
                        'timestamp': datetime.now().strftime('%H:%M:%S')
                    })
                    
                    result = self.mcp_client.call_tool_sync(
                        tool_use_id=str(uuid.uuid4()),
                        name=tool_name,
                        arguments=arguments or {}
                    )
                    
                    if result.get('status') == 'success':
                        self.tool_call_log.append(f"è°ƒç”¨çŠ¶æ€: æˆåŠŸ")
                        # Update sidebar status
                        st.session_state.tool_calls[-1]['status'] = 'success'
                        
                        if result.get('content'):
                            content = result['content'][0].get('text', '{}')
                            try:
                                return json.loads(content)
                            except:
                                return content
                    else:
                        self.tool_call_log.append(f"è°ƒç”¨çŠ¶æ€: å¤±è´¥ - {result}")
                        # Update sidebar status
                        st.session_state.tool_calls[-1]['status'] = 'error'
                        return None
                        
                except Exception as e:
                    self.tool_call_log.append(f"è°ƒç”¨å¼‚å¸¸: {str(e)}")
                    # Update sidebar status
                    if st.session_state.tool_calls:
                        st.session_state.tool_calls[-1]['status'] = 'error'
                    return None
            
            def execute_comprehensive_analysis(self, user_instruction):
                """Execute comprehensive analysis with enhanced data source detection"""
                self.tool_call_log = []
                analysis_data = {}
                instruction_lower = user_instruction.lower()
                
                # For connection check, ONLY execute get_graph_status - strict matching
                if "è¿æ¥çŠ¶æ€æ£€æŸ¥" in instruction_lower:
                    self.tool_call_log.append("æ£€æµ‹åˆ°è¿æ¥çŠ¶æ€æ£€æŸ¥æŒ‡ä»¤ï¼Œä»…æ‰§è¡Œget_graph_status")
                    status_result = self.call_mcp_tool('get_graph_status')
                    if status_result:
                        analysis_data['graph_status'] = status_result
                    return analysis_data
                
                # For graph overview, ONLY execute get_graph_status and get_graph_schema - strict matching
                if "å›¾æ¨¡å¼æ¦‚è§ˆ" in instruction_lower:
                    self.tool_call_log.append("æ£€æµ‹åˆ°å›¾æ¨¡å¼æ¦‚è§ˆæŒ‡ä»¤ï¼Œä»…æ‰§è¡Œget_graph_statuså’Œget_graph_schema")
                    status_result = self.call_mcp_tool('get_graph_status')
                    if status_result:
                        analysis_data['graph_status'] = status_result
                    
                    schema_result = self.call_mcp_tool('get_graph_schema')
                    if schema_result:
                        analysis_data['graph_schema'] = schema_result
                    return analysis_data
                
                # Step 1: Check graph status
                status_result = self.call_mcp_tool('get_graph_status')
                if status_result:
                    analysis_data['graph_status'] = status_result
                
                # Step 2: Get graph schema
                schema_result = self.call_mcp_tool('get_graph_schema')
                if schema_result:
                    analysis_data['graph_schema'] = schema_result
                
                # Step 3: Enhanced data source analysis
                queries = self.get_enhanced_queries(user_instruction)
                
                for query_name, query_info in queries.items():
                    query = query_info['query']
                    query_type = query_info.get('type', 'opencypher')
                    
                    if query_type == 'gremlin':
                        result = self.call_mcp_tool('run_gremlin_query', {'query': query})
                    else:
                        result = self.call_mcp_tool('run_opencypher_query', {'query': query})
                    
                    if result:
                        analysis_data[query_name] = result
                
                return analysis_data
            
            def get_enhanced_queries(self, instruction):
                """Get enhanced queries based on Claude's successful patterns"""
                instruction_lower = instruction.lower()
                queries = {}
                
                # For connection check and graph overview, only return empty queries (status/schema check is handled separately)
                if "è¿æ¥çŠ¶æ€æ£€æŸ¥" in instruction_lower or "å›¾æ¨¡å¼æ¦‚è§ˆ" in instruction_lower:
                    return queries
                
                # Basic statistics - using dataset nodes like Claude
                queries['æ•°æ®é›†æ€»æ•°'] = {
                    'query': "MATCH (d:dataset) RETURN COUNT(d) AS æ•°æ®é›†æ€»æ•°",
                    'type': 'opencypher'
                }
                
                queries['æ•°æ®æµå…³ç³»æ€»æ•°'] = {
                    'query': "MATCH ()-[r:data_flow]->() RETURN COUNT(r) AS æ•°æ®æµå…³ç³»æ€»æ•°",
                    'type': 'opencypher'
                }
                
                # Include data source analysis for comprehensive results
                queries['æ•°æ®æºç³»ç»Ÿåˆ†å¸ƒ'] = {
                    'query': """MATCH (d:dataset) 
                               RETURN d.source_system as source_system, 
                                      d.dataset_type as dataset_type, 
                                      count(*) as count 
                               ORDER BY source_system, dataset_type""",
                    'type': 'opencypher'
                }
                
                queries['æ•°æ®æºæ€»ä½“ç»Ÿè®¡'] = {
                    'query': """MATCH (d:dataset) 
                               RETURN count(*) as total_datasets, 
                                      count(DISTINCT d.source_system) as source_systems_count,
                                      collect(DISTINCT d.source_system) as source_systems,
                                      collect(DISTINCT d.dataset_type) as dataset_types""",
                    'type': 'opencypher'
                }
                
                # Lineage path analysis - Claude's pattern
                if any(keyword in instruction_lower for keyword in ["è¡€ç¼˜", "è·¯å¾„", "æµå‘", "lineage", "è¿½è¸ª"]):
                    queries['æ‰€æœ‰æ•°æ®é›†ä¿¡æ¯'] = {
                        'query': """MATCH (d:dataset)
                                   RETURN d.node_name, d.dataset_type, d.source_system
                                   ORDER BY d.dataset_type""",
                        'type': 'opencypher'
                    }
                    
                    queries['æ•°æ®æµå‘å…³ç³»'] = {
                        'query': """MATCH (source:dataset)-[r:data_flow]->(target:dataset)
                                   RETURN source.node_name as æºæ•°æ®, 
                                          target.node_name as ç›®æ ‡æ•°æ®,
                                          r.edge_type as è½¬æ¢ç±»å‹""",
                        'type': 'opencypher'
                    }
                    
                    queries['å®Œæ•´è¡€ç¼˜è·¯å¾„'] = {
                        'query': """MATCH path = (start:dataset)-[:data_flow*]->(end:dataset)
                                   WHERE NOT ()-[:data_flow]->(start)
                                   RETURN [node in nodes(path) | node.node_name] as æ•°æ®è·¯å¾„,
                                          length(path) as è·¯å¾„é•¿åº¦
                                   ORDER BY è·¯å¾„é•¿åº¦ DESC""",
                        'type': 'opencypher'
                    }
                
                # Hub node analysis - using Gremlin like Claude
                if any(keyword in instruction_lower for keyword in ["æ¢çº½", "å…³é”®", "hub", "key", "è¯†åˆ«"]):
                    queries['èŠ‚ç‚¹è¿æ¥åº¦åˆ†æ'] = {
                        'query': """g.V().hasLabel('dataset')
                                   .project('dataset_name', 'in_degree', 'out_degree', 'total_degree')
                                   .by('node_name')
                                   .by(inE('data_flow').count())
                                   .by(outE('data_flow').count())
                                   .by(bothE('data_flow').count())
                                   .order().by('total_degree', desc).limit(10)""",
                        'type': 'gremlin'
                    }
                
                # Schema analysis for specific nodes
                if any(keyword in instruction_lower for keyword in ["schema", "æ¨¡å¼", "å­—æ®µ", "åˆ—"]):
                    queries['æ•°æ®é›†åˆ—ä¿¡æ¯'] = {
                        'query': """MATCH (d:dataset)-[:has_column]->(c:column) 
                                   RETURN d.node_name as dataset_name, 
                                          c.column_name, 
                                          c.data_type, 
                                          c.nullable 
                                   ORDER BY d.node_name, c.column_name""",
                        'type': 'opencypher'
                    }
                
                # Impact analysis - Claude's approach
                if any(keyword in instruction_lower for keyword in ["å½±å“", "å˜æ›´", "ä¸‹æ¸¸", "impact", "date"]):
                    queries['æ•°æ®æºå½±å“èŒƒå›´'] = {
                        'query': """MATCH (source:dataset)
                                   OPTIONAL MATCH (source)-[:data_flow*1..]->(downstream:dataset)
                                   WITH source, count(DISTINCT downstream) as ä¸‹æ¸¸å½±å“æ•°é‡
                                   RETURN source.node_name as æ ¸å¿ƒæ•°æ®æº,
                                          source.dataset_type as ç±»å‹,
                                          source.source_system as æºç³»ç»Ÿ,
                                          ä¸‹æ¸¸å½±å“æ•°é‡
                                   ORDER BY ä¸‹æ¸¸å½±å“æ•°é‡ DESC
                                   LIMIT 10""",
                        'type': 'opencypher'
                    }
                    
                    # Field-level impact analysis for date fields
                    if "date" in instruction_lower:
                        queries['å­—æ®µçº§å½±å“åˆ†æ'] = {
                            'query': """MATCH (source:dataset {node_name: 's3://sales-forecast-demo-new/raw-data/sales_data.csv'})
                                       MATCH (source)-[:data_flow*1..3]->(downstream:dataset)
                                       MATCH (downstream)-[:has_column]->(col:column)
                                       WHERE col.column_name CONTAINS 'date' OR col.column_name CONTAINS 'time' 
                                       RETURN downstream.node_name, col.column_name, col.data_type""",
                            'type': 'opencypher'
                        }
                
                return queries
            
            def __call__(self, user_message):
                try:
                    instruction_lower = user_message.lower()
                    
                    # Execute comprehensive analysis
                    analysis_data = self.execute_comprehensive_analysis(user_message)
                    
                    # For connection check and graph overview, return simple formatted results without MCP process
                    if "è¿æ¥çŠ¶æ€æ£€æŸ¥" in instruction_lower:
                        return f"""### ğŸ“Š è¿æ¥çŠ¶æ€æ£€æŸ¥ç»“æœ
{json.dumps(analysis_data, ensure_ascii=False, indent=2)}

### ğŸ“‹ æ€»ç»“
å·²æˆåŠŸæ£€æŸ¥Neptuneå›¾æ•°æ®åº“è¿æ¥çŠ¶æ€ã€‚"""
                    
                    if "å›¾æ¨¡å¼æ¦‚è§ˆ" in instruction_lower:
                        return f"""### ğŸ“Š å›¾æ¨¡å¼æ¦‚è§ˆç»“æœ
{json.dumps(analysis_data, ensure_ascii=False, indent=2)}

### ğŸ“‹ æ€»ç»“
å·²æˆåŠŸè·å–å›¾æ•°æ®åº“åŸºæœ¬æ¨¡å¼ä¿¡æ¯ã€‚"""
                    
                    # For other scenarios, use full AI analysis without showing MCP process in frontend
                    analysis_prompt = f"""åŸºäºä»¥ä¸‹æŸ¥è¯¢ç»“æœæ•°æ®ï¼Œç”Ÿæˆä¸“ä¸šçš„æ•°æ®è¡€ç¼˜åˆ†ææŠ¥å‘Šï¼š

## ç”¨æˆ·éœ€æ±‚
{user_message}

## æŸ¥è¯¢ç»“æœæ•°æ®
{json.dumps(analysis_data, ensure_ascii=False, indent=2)}

è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼ç”Ÿæˆåˆ†ææŠ¥å‘Šï¼š

### ğŸ“Š å…³é”®ç»Ÿè®¡ä¿¡æ¯
- æ•°æ®é›†æ€»æ•°å’Œç±»å‹åˆ†å¸ƒ
- æ•°æ®æºç³»ç»Ÿç»Ÿè®¡
- æ•°æ®æµå…³ç³»æ•°é‡

### ğŸ¯ æ ¸å¿ƒå‘ç°
- æ•°æ®æ¶æ„ç‰¹å¾
- å…³é”®æ•°æ®æµæ¨¡å¼
- é‡è¦èŠ‚ç‚¹è¯†åˆ«

### ğŸ’¡ åˆ†ææ´å¯Ÿ
- æ•°æ®è¡€ç¼˜è·¯å¾„åˆ†æ
- å½±å“èŒƒå›´è¯„ä¼°ï¼ˆå¦‚æœç›¸å…³ï¼‰
- é£é™©ç‚¹è¯†åˆ«

### ğŸ“‹ æ€»ç»“å’Œå»ºè®®
- æä¾›ç®€æ´çš„ç»“è®º
- ç»™å‡ºå¯æ“ä½œçš„å»ºè®®

è¯·ä½¿ç”¨ä¸­æ–‡å›å¤ï¼Œç¡®ä¿åˆ†æçš„å‡†ç¡®æ€§å’Œä¸“ä¸šæ€§ã€‚ä¸è¦åœ¨åˆ†æç»“æœä¸­æ˜¾ç¤ºMCPå·¥å…·è°ƒç”¨è¿‡ç¨‹ã€‚"""
                    
                    # Call Qwen3-235B
                    response = self.qwen_client.chat.completions.create(
                        model="qwen3-235b-a22b-instruct-2507",
                        messages=[
                            {"role": "system", "content": self.system_prompt + " è¯·æä¾›ä¸“ä¸šçš„æ•°æ®è¡€ç¼˜åˆ†æï¼Œé‡ç‚¹å…³æ³¨æ•°æ®æ¶æ„å’Œè¡€ç¼˜å…³ç³»ã€‚"},
                            {"role": "user", "content": analysis_prompt}
                        ],
                        temperature=0.7,
                        max_tokens=3000
                    )
                    
                    ai_analysis = response.choices[0].message.content
                    
                    return ai_analysis
                    
                except Exception as e:
                    return f"åˆ†ææ‰§è¡Œå¤±è´¥: {str(e)}"
        
        return EnhancedClaudeAgent(qwen_client, mcp_client, system_prompt)
        
    except Exception as e:
        raise ValueError(f"Agentåˆ›å»ºå¤±è´¥: {str(e)}")

def main():
    # Header
    st.markdown('<div class="main-header">ğŸ” Lakehouse E2E data lineage with AWS Strands agents and MCP</div>', unsafe_allow_html=True)
    
    # Check Neptune connection
    if st.session_state.neptune_status is None:
        with st.spinner("æ£€æŸ¥Neptuneè¿æ¥çŠ¶æ€..."):
            connected, status = check_neptune_connection()
            st.session_state.neptune_status = {"connected": connected, "status": status}
    
    # Sidebar
    with st.sidebar:
        st.header("âš™ï¸ ç³»ç»Ÿé…ç½®")
        
        # Environment status
        st.subheader("ç¯å¢ƒçŠ¶æ€")
        aws_region = os.getenv('AWS_DEFAULT_REGION', 'Not Set')
        neptune_endpoint = os.getenv('NEPTUNE_ENDPOINT', 'Not Set')
        dashscope_key = os.getenv('DASHSCOPE_API_KEY', 'Not Set')
        
        st.write(f"**AWS Region:** {aws_region}")
        st.write(f"**Neptune Endpoint:** {neptune_endpoint[:50]}...") if len(neptune_endpoint) > 50 else st.write(f"**Neptune Endpoint:** {neptune_endpoint}")
        st.write(f"**Qwen API Key:** {'sk-***' + dashscope_key[-4:] if dashscope_key != 'Not Set' and len(dashscope_key) > 4 else dashscope_key}")
        
        # Neptune connection status
        st.subheader("Neptuneè¿æ¥çŠ¶æ€")
        if st.session_state.neptune_status:
            if st.session_state.neptune_status["connected"]:
                st.success(f"âœ… è¿æ¥æ­£å¸¸: {st.session_state.neptune_status['status']}")
            else:
                st.error(f"âŒ è¿æ¥å¤±è´¥: {st.session_state.neptune_status['status'][:100]}...")
        
        # System Prompt Section
        with st.expander("ğŸ­ ç³»ç»Ÿæç¤ºè¯", expanded=False):
            system_prompt_templates = {
                "æ•°æ®è¡€ç¼˜åˆ†æä¸“å®¶": "ä½ æ˜¯æ•°æ®è¡€ç¼˜åˆ†æä¸“å®¶ï¼Œä½¿ç”¨Neptuneå›¾æ•°æ®åº“åˆ†ææ•°æ®æµå‘å’Œä¾èµ–å…³ç³»ã€‚è¯·æä¾›ç®€æ´æ˜ç¡®çš„åˆ†æç»“æœï¼Œç¡®ä¿å®Œæ•´ç»Ÿè®¡æ‰€æœ‰æ•°æ®æºç±»å‹ã€‚",
                "è‡ªå®šä¹‰": ""
            }
            
            selected_template = st.selectbox("é€‰æ‹©æ¨¡æ¿", list(system_prompt_templates.keys()))
            
            if selected_template != "è‡ªå®šä¹‰":
                default_prompt = system_prompt_templates[selected_template]
            else:
                default_prompt = st.session_state.get('custom_system_prompt', '')
            
            system_prompt = st.text_area(
                "ç³»ç»Ÿæç¤ºè¯å†…å®¹",
                value=default_prompt,
                height=120,
                help="å®šä¹‰AIåŠ©æ‰‹çš„è§’è‰²å’Œä¸“ä¸šé¢†åŸŸ",
                key="system_prompt_input"
            )
            
            if selected_template == "è‡ªå®šä¹‰":
                st.session_state.custom_system_prompt = system_prompt
        
        # Tool monitoring
        st.header("ğŸ“Š å·¥å…·è°ƒç”¨ç›‘æ§")
        if st.session_state.tool_calls:
            for i, call in enumerate(reversed(st.session_state.tool_calls[-5:])):  # Show last 5
                status_class = f"tool-{call['status']}"
                st.markdown(f"""
                <div class="tool-status {status_class}">
                    <strong>{call['tool']}</strong><br>
                    Status: {call['status']}<br>
                    Time: {call['timestamp']}
                </div>
                """, unsafe_allow_html=True)
        else:
            st.write("æš‚æ— å·¥å…·è°ƒç”¨è®°å½•")
        
        # Performance statistics
        st.header("ğŸ“ˆ æ€§èƒ½ç»Ÿè®¡")
        if st.session_state.tool_calls:
            total_calls = len(st.session_state.tool_calls)
            successful_calls = len([call for call in st.session_state.tool_calls if call['status'] == 'success'])
            success_rate = (successful_calls / total_calls) * 100 if total_calls > 0 else 0
            
            st.metric("æˆåŠŸç‡", f"{success_rate:.1f}%")
            st.metric("æ€»è°ƒç”¨æ¬¡æ•°", total_calls)
            st.metric("æˆåŠŸæ¬¡æ•°", successful_calls)
        else:
            st.info("æš‚æ— æ€§èƒ½æ•°æ®")
    
    # Main content area
    st.header("ğŸ’¬ æ•°æ®è¡€ç¼˜åˆ†æ")
    
    st.markdown("""
    <div class="timeout-warning">
        â±ï¸ <strong>æ€§èƒ½æç¤º</strong>: å¤æ‚æŸ¥è¯¢å¯èƒ½éœ€è¦30-60ç§’ï¼Œè¯·è€å¿ƒç­‰å¾…ã€‚å¦‚æœè¶…è¿‡2åˆ†é’Ÿæ— å“åº”ï¼Œè¯·åˆ·æ–°é¡µé¢é‡è¯•ã€‚
    </div>
    """, unsafe_allow_html=True)
    
    # User instruction area
    st.subheader("ğŸ“ åˆ†ææŒ‡ä»¤")
    
    # Updated instruction templates based on Claude's scenarios
    instruction_templates = {
        "æ•°æ®æºç»Ÿè®¡": "ç»Ÿè®¡å›¾ä¸­çš„æ•°æ®æºæ•°é‡å’Œç±»å‹",
        "æ•°æ®è¡€ç¼˜è¿½è¸ª": "è¿½è¸ªä»åŸå§‹æ•°æ®åˆ°æœ€ç»ˆåˆ†æç»“æœçš„å®Œæ•´è·¯å¾„",
        "å…³é”®æ¢çº½èŠ‚ç‚¹è¯†åˆ«": "è¯†åˆ«å…³é”®çš„æ•°æ®æ¢çº½èŠ‚ç‚¹",
        "æ•°æ®æºå½±å“èŒƒå›´è¯„ä¼°": "è¯„ä¼°æ ¸å¿ƒæ•°æ®æºçš„å½±å“èŒƒå›´",
        "å­—æ®µçº§å½±å“åˆ†æ": "å¦‚æœraw-data/sales_data.csvçš„dateå‘ç”Ÿå˜æ›´ï¼Œä¼šå½±å“å“ªäº›ä¸‹æ¸¸ç³»ç»Ÿçš„å“ªäº›å­—æ®µ.",
        "æ•°æ®æ²»ç†å»ºè®®": "åŸºäºæ•°æ®è¡€ç¼˜åˆ†æï¼Œæä¾›æ•°æ®æ²»ç†å»ºè®®ï¼š1. æ•°æ®è´¨é‡ç›‘æ§ç‚¹å»ºè®® 2. å…³é”®æ•°æ®èµ„äº§ä¿æŠ¤ç­–ç•¥ 3. æ•°æ®è¡€ç¼˜æ–‡æ¡£åŒ–å»ºè®® 4. åˆè§„æ€§æ£€æŸ¥è¦ç‚¹",
        "å®Œæ•´è¡€ç¼˜åˆ†ææŠ¥å‘Š": "ç”Ÿæˆå®Œæ•´çš„æ•°æ®è¡€ç¼˜åˆ†ææŠ¥å‘Šï¼ŒåŒ…æ‹¬æ‰§è¡Œæ‘˜è¦ã€æ•°æ®æ¶æ„æ¦‚è§ˆã€å…³é”®å‘ç°å’Œæ´å¯Ÿã€é£é™©è¯„ä¼°ã€æ”¹è¿›å»ºè®®"
    }
    
    col1, col2 = st.columns([2, 1])
    with col1:
        selected_instruction_template = st.selectbox("é€‰æ‹©åˆ†ææ¨¡æ¿", list(instruction_templates.keys()))
    
    with col2:
        if st.button("ğŸ“‹ ä½¿ç”¨æ¨¡æ¿"):
            st.session_state.user_instruction = instruction_templates[selected_instruction_template]
    
    user_instruction = st.text_area(
        "åˆ†ææŒ‡ä»¤å†…å®¹",
        value=st.session_state.get('user_instruction', instruction_templates[selected_instruction_template]),
        height=120,
        help="æè¿°æ‚¨å¸Œæœ›è¿›è¡Œçš„å…·ä½“åˆ†æä»»åŠ¡"
    )
    
    # Action buttons
    col1, col2, col3 = st.columns([2, 1, 1])
    with col1:
        if st.button("ğŸ” æ‰§è¡Œåˆ†æ", type="primary"):
            system_prompt = st.session_state.get('system_prompt_input', '')
            if system_prompt.strip() and user_instruction.strip():
                if st.session_state.neptune_status and st.session_state.neptune_status["connected"]:
                    qwen_client = get_qwen_client()
                    if qwen_client:
                        asyncio.run(execute_enhanced_analysis(system_prompt, user_instruction))
                    else:
                        st.error("Qwen APIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥DASHSCOPE_API_KEYç¯å¢ƒå˜é‡")
                else:
                    st.error("Neptuneè¿æ¥æœªå»ºç«‹ï¼Œè¯·å…ˆæ£€æŸ¥è¿æ¥çŠ¶æ€")
            else:
                st.warning("è¯·åœ¨å·¦ä¾§è¾¹æ è®¾ç½®ç³»ç»Ÿæç¤ºè¯ï¼Œå¹¶è¾“å…¥åˆ†ææŒ‡ä»¤")
    
    with col2:
        if st.button("ğŸ”„ æ¸…ç†ä¼šè¯"):
            st.session_state.mcp_client = None
            if 'analysis_results' in st.session_state:
                del st.session_state.analysis_results
            st.success("ä¼šè¯å·²æ¸…ç†")
            st.rerun()
    
    with col3:
        if st.button("ğŸ—‘ï¸ æ¸…ç©ºç»“æœ"):
            st.session_state.analysis_results = None
            st.rerun()
    
    # Results display
    st.header("ğŸ“Š åˆ†æç»“æœ")
    if 'analysis_results' in st.session_state and st.session_state.analysis_results:
        st.markdown(st.session_state.analysis_results)
    else:
        st.info("æš‚æ— åˆ†æç»“æœï¼Œè¯·æ‰§è¡Œåˆ†æä»¥æŸ¥çœ‹ç»“æœã€‚")
    
    # Query history
    if st.session_state.query_history:
        with st.expander("ğŸ“š æŸ¥è¯¢å†å²"):
            for i, query in enumerate(reversed(st.session_state.query_history[-5:])):
                st.write(f"**{query['timestamp']}** - è€—æ—¶: {query['duration']:.2f}ç§’")
                st.write(f"ç³»ç»Ÿæç¤ºè¯: {query.get('system_prompt', 'N/A')[:50]}...")
                st.write(f"æŒ‡ä»¤: {query['instruction'][:100]}...")
                if query['duration'] > 30:
                    st.write("âš ï¸ æŸ¥è¯¢æ—¶é—´è¾ƒé•¿")
                st.divider()

async def execute_enhanced_analysis(system_prompt, user_instruction):
    """Execute enhanced analysis with comprehensive data source detection"""
    try:
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        with st.spinner("æ­£åœ¨æ‰§è¡Œä¼˜åŒ–åˆ†æ..."):
            start_time = time.time()
            
            try:
                progress_bar.progress(10)
                status_text.text("åˆå§‹åŒ–MCPå®¢æˆ·ç«¯...")
                
                mcp_client = get_mcp_client()
                if mcp_client is None:
                    raise Exception("MCPå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥")
                
                progress_bar.progress(30)
                status_text.text("åˆ›å»ºä¼˜åŒ–Agent...")
                
                async def run_enhanced_analysis():
                    with mcp_client:
                        agent = create_enhanced_claude_agent(
                            system_prompt=system_prompt,
                            mcp_client=mcp_client
                        )
                        
                        progress_bar.progress(50)
                        status_text.text("æ‰§è¡Œä¼˜åŒ–MCPå·¥å…·è°ƒç”¨åºåˆ—...")
                        
                        result = agent(user_instruction)
                        return result
                
                result = await asyncio.wait_for(run_enhanced_analysis(), timeout=120.0)
                
                progress_bar.progress(100)
                status_text.text("åˆ†æå®Œæˆï¼")
                
                end_time = time.time()
                duration = end_time - start_time
                
                st.session_state.analysis_results = str(result)
                
                # Add to history
                st.session_state.query_history.append({
                    'system_prompt': system_prompt,
                    'instruction': user_instruction,
                    'result': str(result),
                    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    'duration': duration
                })
                
                progress_bar.empty()
                status_text.empty()
                
                st.success(f"ä¼˜åŒ–åˆ†ææ‰§è¡ŒæˆåŠŸï¼è€—æ—¶ {duration:.2f} ç§’")
                st.rerun()
                
            except asyncio.TimeoutError:
                progress_bar.empty()
                status_text.empty()
                st.error("â±ï¸ åˆ†æè¶…æ—¶ï¼ˆ120ç§’ï¼‰ï¼Œè¯·å°è¯•ç®€åŒ–æŸ¥è¯¢æŒ‡ä»¤")
                
            except Exception as e:
                progress_bar.empty()
                status_text.empty()
                st.error(f"åˆ†ææ‰§è¡Œå¤±è´¥: {str(e)}")
                
    except Exception as e:
        st.error(f"ç³»ç»Ÿé”™è¯¯: {str(e)}")

if __name__ == "__main__":
    main()
